# Fairness in Machine Learning Crash Course

-----------------------------------

_Collection of notes taken while reviewing the materials in Google's 60-minute AI Ethics crash course. If you are interested in reviewing the material yourself (and I highly recommend it!), check out the course website [here](https://developers.google.com/machine-learning/crash-course/fairness/video-lecture)._

-----------------------------------

## Video Lecture
Specific learning objectives of this module include:

* Become aware of common human biases that can inadvertently be reproduced by machine learning algorithms.
* Proactively explore data to identify sources of bias before training a model.
* Evaluate model predictions for bias.

### Collecting and annotating training data.

#### Human Biases in Data
These examples highlight unconscious bias from _the world_ that we might reflect in machine learning when using some of the world's data.

* **Reporting Bias**: For example, not knowing that yellow is a good descriptor for banana because most humans would not tag a training image for the fruit as "yellow banana".

* **Selection bias**: Data does not come from a representative sample of a population.

* **Overgeneralization**: When a conclusion is made that is limited, or not specific enough.

* **Out-group homogeneity bias**: We tend to assume that people and groups we don't interact with every day are somehow more similar to one another, more homogeneous, and less nuanced than those in our in-group.

#### Human Biases in Data
These examples highlight unconscious bias in our procedures that we might reflect in our machine learning modeling.

* **Confirmation bias**: Tendency to have confidence in data that supports pre-existing ideas and beliefs.

* **Auomation bias**: Preference to use systems that are automated, such as machine learning systems, even though there are biases baked into these tools.

### Designing for Fairness

1. _Consider the problem._ What's being represented in your data, and what might be overlooked?

2. _Ask experts._ They might provide guidance or suggestions, which could help your project have a more long-term, positive impact.

3. _Train the models to account for bias._ What do outliers look like, and how does your model handle them? What implicit assumptions is your model making, and how can you intperpret or mitigate those?

4. _Interpret outcomes._ Is the machine learning system overgeneralizing? If a human was charged with completing the task, what would appropriate social behavior look like?

5. _Publish with context._ Are you sharing examples? What might effects be?

## Types of Bias

## Identifying Bias

## Evaluating for Bias

## Programming Exercise

## Check Your Understanding
