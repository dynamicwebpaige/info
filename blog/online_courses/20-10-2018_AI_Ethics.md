# Fairness in Machine Learning Crash Course

-----------------------------------

_Collection of notes taken while reviewing the materials in Google's 60-minute AI Ethics crash course. If you are interested in reviewing the material yourself (and I highly recommend it!), check out the course website [here](https://developers.google.com/machine-learning/crash-course/fairness/video-lecture)._

-----------------------------------

## Video Lecture
Specific learning objectives of this module include:

* Become aware of common human biases that can inadvertently be reproduced by machine learning algorithms.
* Proactively explore data to identify sources of bias before training a model.
* Evaluate model predictions for bias.

### Collecting and annotating training data.

#### Human Biases in Data
These examples highlight unconscious bias from _the world_ that we might reflect in machine learning when using some of the world's data.

* **Reporting Bias**: For example, not knowing that yellow is a good descriptor for banana because most humans would not tag a training image for the fruit as "yellow banana".

* **Selection bias**: Data does not come from a representative sample of a population.

* **Overgeneralization**: When a conclusion is made that is limited, or not specific enough.

* **Out-group homogeneity bias**: We tend to assume that people and groups we don't interact with every day are somehow more similar to one another, more homogeneous, and less nuanced than those in our in-group.

#### Human Biases in Data
These examples highlight unconscious bias in our procedures that we might reflect in our machine learning modeling.

* **Confirmation bias**: Tendency to have confidence in data that supports pre-existing ideas and beliefs.

* **Auomation bias**: Preference to use systems that are automated, such as machine learning systems, even though there are biases baked into these tools.

### Designing for Fairness

1. _Consider the problem._ What's being represented in your data, and what might be overlooked?

2. _Ask experts._ They might provide guidance or suggestions, which could help your project have a more long-term, positive impact.

3. _Train the models to account for bias._ What do outliers look like, and how does your model handle them? What implicit assumptions is your model making, and how can you intperpret or mitigate those?

4. _Interpret outcomes._ Is the machine learning system overgeneralizing? If a human was charged with completing the task, what would appropriate social behavior look like?

5. _Publish with context._ Are you sharing examples? What might effects be?

## Types of Bias

**Primary takeaway**: Engineering models are not objective; they are based on data that is curated, entered, or provisioned by humans.

* [List of Cognitive Biases (over 100!)](https://en.wikipedia.org/wiki/List_of_cognitive_biases)

### Reporting Bias
Occurs when the frequency of events, properties, and/or outcomes captured in a data set does not accurately reflect their real-world frequency (people only take note of what is out-of-the-ordinary).

_An example_: skewed book reviews - either extremely positive, or extremely negative.

### Automation Bias
Tendency to favor results generated by automated systems over those generated by non-automated systems, irrespective of the error rates of each.

### Selection Bias
Occurs if a data set's examples are chosen in a way that is not reflective of their real-world distribution.

* **Coverage bias**: Data is not selected in a representative fashion (ex: landline phone surveys).

* **Non-response bias** (or **participation bias**): Data ends up being unrepresentative due to participation gaps in the data-collection process.

* **Sampling bias**: Proper randomization is not used during data collection (ex: taking only the first 200 survey responses).

### Group Attribution Bias
Tendency to generalize what is true of individuals to an entire group to which they belong.

* **In-group bias**: A preference for members of a group to which _you also_ belong, or for characteristics that you also share.

* **Out-group homogeneity bias**: A tendency to stereotype individual members of a group to which _you do not_ belong, or to see their characteristics as more uniform.

### Implicit Bias
Occurs when assumptions are made based on one's own mental models and personal experiences that do not necessary apply more generally.

* **Confirmation bias**: Model builders unconsciously process data in ways that affirm preexisting beliefs and hypotheses

* **Experimenter's bias**: Model builder keeps training a model until it produces a result that aligns with their original hypothesis.

## Identifying Bias
Some red flags for sneaky biases in data:

* Missing feature values.
* Unexpected feature values.
* _Data skew_ - where certain groups or characteristics might be over- or under-represented in your model.

## Evaluating for Bias

## Programming Exercise

## Check Your Understanding
